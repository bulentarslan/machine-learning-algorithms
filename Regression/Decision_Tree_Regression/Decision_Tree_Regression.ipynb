{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Position_Salaries.csv')\n",
    "X = data.iloc[: , 1:2].values\n",
    "y = data.iloc[: , -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the descision tree regression to dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):\\n',\n",
       "  '    \"\"\"A decision tree regressor.\\n',\n",
       "  '\\n',\n",
       "  '    Read more in the :ref:`User Guide <tree>`.\\n',\n",
       "  '\\n',\n",
       "  '    Parameters\\n',\n",
       "  '    ----------\\n',\n",
       "  '    criterion : string, optional (default=\"mse\")\\n',\n",
       "  '        The function to measure the quality of a split. Supported criteria\\n',\n",
       "  '        are \"mse\" for the mean squared error, which is equal to variance\\n',\n",
       "  '        reduction as feature selection criterion and minimizes the L2 loss\\n',\n",
       "  '        using the mean of each terminal node, \"friedman_mse\", which uses mean\\n',\n",
       "  \"        squared error with Friedman's improvement score for potential splits,\\n\",\n",
       "  '        and \"mae\" for the mean absolute error, which minimizes the L1 loss\\n',\n",
       "  '        using the median of each terminal node.\\n',\n",
       "  '\\n',\n",
       "  '        .. versionadded:: 0.18\\n',\n",
       "  '           Mean Absolute Error (MAE) criterion.\\n',\n",
       "  '\\n',\n",
       "  '    splitter : string, optional (default=\"best\")\\n',\n",
       "  '        The strategy used to choose the split at each node. Supported\\n',\n",
       "  '        strategies are \"best\" to choose the best split and \"random\" to choose\\n',\n",
       "  '        the best random split.\\n',\n",
       "  '\\n',\n",
       "  '    max_depth : int or None, optional (default=None)\\n',\n",
       "  '        The maximum depth of the tree. If None, then nodes are expanded until\\n',\n",
       "  '        all leaves are pure or until all leaves contain less than\\n',\n",
       "  '        min_samples_split samples.\\n',\n",
       "  '\\n',\n",
       "  '    min_samples_split : int, float, optional (default=2)\\n',\n",
       "  '        The minimum number of samples required to split an internal node:\\n',\n",
       "  '\\n',\n",
       "  '        - If int, then consider `min_samples_split` as the minimum number.\\n',\n",
       "  '        - If float, then `min_samples_split` is a percentage and\\n',\n",
       "  '          `ceil(min_samples_split * n_samples)` are the minimum\\n',\n",
       "  '          number of samples for each split.\\n',\n",
       "  '\\n',\n",
       "  '        .. versionchanged:: 0.18\\n',\n",
       "  '           Added float values for percentages.\\n',\n",
       "  '\\n',\n",
       "  '    min_samples_leaf : int, float, optional (default=1)\\n',\n",
       "  '        The minimum number of samples required to be at a leaf node:\\n',\n",
       "  '\\n',\n",
       "  '        - If int, then consider `min_samples_leaf` as the minimum number.\\n',\n",
       "  '        - If float, then `min_samples_leaf` is a percentage and\\n',\n",
       "  '          `ceil(min_samples_leaf * n_samples)` are the minimum\\n',\n",
       "  '          number of samples for each node.\\n',\n",
       "  '\\n',\n",
       "  '        .. versionchanged:: 0.18\\n',\n",
       "  '           Added float values for percentages.\\n',\n",
       "  '\\n',\n",
       "  '    min_weight_fraction_leaf : float, optional (default=0.)\\n',\n",
       "  '        The minimum weighted fraction of the sum total of weights (of all\\n',\n",
       "  '        the input samples) required to be at a leaf node. Samples have\\n',\n",
       "  '        equal weight when sample_weight is not provided.\\n',\n",
       "  '\\n',\n",
       "  '    max_features : int, float, string or None, optional (default=None)\\n',\n",
       "  '        The number of features to consider when looking for the best split:\\n',\n",
       "  '\\n',\n",
       "  '        - If int, then consider `max_features` features at each split.\\n',\n",
       "  '        - If float, then `max_features` is a percentage and\\n',\n",
       "  '          `int(max_features * n_features)` features are considered at each\\n',\n",
       "  '          split.\\n',\n",
       "  '        - If \"auto\", then `max_features=n_features`.\\n',\n",
       "  '        - If \"sqrt\", then `max_features=sqrt(n_features)`.\\n',\n",
       "  '        - If \"log2\", then `max_features=log2(n_features)`.\\n',\n",
       "  '        - If None, then `max_features=n_features`.\\n',\n",
       "  '\\n',\n",
       "  '        Note: the search for a split does not stop until at least one\\n',\n",
       "  '        valid partition of the node samples is found, even if it requires to\\n',\n",
       "  '        effectively inspect more than ``max_features`` features.\\n',\n",
       "  '\\n',\n",
       "  '    random_state : int, RandomState instance or None, optional (default=None)\\n',\n",
       "  '        If int, random_state is the seed used by the random number generator;\\n',\n",
       "  '        If RandomState instance, random_state is the random number generator;\\n',\n",
       "  '        If None, the random number generator is the RandomState instance used\\n',\n",
       "  '        by `np.random`.\\n',\n",
       "  '\\n',\n",
       "  '    max_leaf_nodes : int or None, optional (default=None)\\n',\n",
       "  '        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\\n',\n",
       "  '        Best nodes are defined as relative reduction in impurity.\\n',\n",
       "  '        If None then unlimited number of leaf nodes.\\n',\n",
       "  '\\n',\n",
       "  '    min_impurity_decrease : float, optional (default=0.)\\n',\n",
       "  '        A node will be split if this split induces a decrease of the impurity\\n',\n",
       "  '        greater than or equal to this value.\\n',\n",
       "  '\\n',\n",
       "  '        The weighted impurity decrease equation is the following::\\n',\n",
       "  '\\n',\n",
       "  '            N_t / N * (impurity - N_t_R / N_t * right_impurity\\n',\n",
       "  '                                - N_t_L / N_t * left_impurity)\\n',\n",
       "  '\\n',\n",
       "  '        where ``N`` is the total number of samples, ``N_t`` is the number of\\n',\n",
       "  '        samples at the current node, ``N_t_L`` is the number of samples in the\\n',\n",
       "  '        left child, and ``N_t_R`` is the number of samples in the right child.\\n',\n",
       "  '\\n',\n",
       "  '        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\\n',\n",
       "  '        if ``sample_weight`` is passed.\\n',\n",
       "  '\\n',\n",
       "  '        .. versionadded:: 0.19\\n',\n",
       "  '\\n',\n",
       "  '    min_impurity_split : float,\\n',\n",
       "  '        Threshold for early stopping in tree growth. A node will split\\n',\n",
       "  '        if its impurity is above the threshold, otherwise it is a leaf.\\n',\n",
       "  '\\n',\n",
       "  '        .. deprecated:: 0.19\\n',\n",
       "  '           ``min_impurity_split`` has been deprecated in favor of\\n',\n",
       "  '           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\\n',\n",
       "  '           Use ``min_impurity_decrease`` instead.\\n',\n",
       "  '\\n',\n",
       "  '    presort : bool, optional (default=False)\\n',\n",
       "  '        Whether to presort the data to speed up the finding of best splits in\\n',\n",
       "  '        fitting. For the default settings of a decision tree on large\\n',\n",
       "  '        datasets, setting this to true may slow down the training process.\\n',\n",
       "  '        When using either a smaller dataset or a restricted depth, this may\\n',\n",
       "  '        speed up the training.\\n',\n",
       "  '\\n',\n",
       "  '    Attributes\\n',\n",
       "  '    ----------\\n',\n",
       "  '    feature_importances_ : array of shape = [n_features]\\n',\n",
       "  '        The feature importances.\\n',\n",
       "  '        The higher, the more important the feature.\\n',\n",
       "  '        The importance of a feature is computed as the\\n',\n",
       "  '        (normalized) total reduction of the criterion brought\\n',\n",
       "  '        by that feature. It is also known as the Gini importance [4]_.\\n',\n",
       "  '\\n',\n",
       "  '    max_features_ : int,\\n',\n",
       "  '        The inferred value of max_features.\\n',\n",
       "  '\\n',\n",
       "  '    n_features_ : int\\n',\n",
       "  '        The number of features when ``fit`` is performed.\\n',\n",
       "  '\\n',\n",
       "  '    n_outputs_ : int\\n',\n",
       "  '        The number of outputs when ``fit`` is performed.\\n',\n",
       "  '\\n',\n",
       "  '    tree_ : Tree object\\n',\n",
       "  '        The underlying Tree object.\\n',\n",
       "  '\\n',\n",
       "  '    Notes\\n',\n",
       "  '    -----\\n',\n",
       "  '    The default values for the parameters controlling the size of the trees\\n',\n",
       "  '    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\\n',\n",
       "  '    unpruned trees which can potentially be very large on some data sets. To\\n',\n",
       "  '    reduce memory consumption, the complexity and size of the trees should be\\n',\n",
       "  '    controlled by setting those parameter values.\\n',\n",
       "  '\\n',\n",
       "  '    The features are always randomly permuted at each split. Therefore,\\n',\n",
       "  '    the best found split may vary, even with the same training data and\\n',\n",
       "  '    ``max_features=n_features``, if the improvement of the criterion is\\n',\n",
       "  '    identical for several splits enumerated during the search of the best\\n',\n",
       "  '    split. To obtain a deterministic behaviour during fitting,\\n',\n",
       "  '    ``random_state`` has to be fixed.\\n',\n",
       "  '\\n',\n",
       "  '    See also\\n',\n",
       "  '    --------\\n',\n",
       "  '    DecisionTreeClassifier\\n',\n",
       "  '\\n',\n",
       "  '    References\\n',\n",
       "  '    ----------\\n',\n",
       "  '\\n',\n",
       "  '    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\\n',\n",
       "  '\\n',\n",
       "  '    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\\n',\n",
       "  '           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\\n',\n",
       "  '\\n',\n",
       "  '    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\\n',\n",
       "  '           Learning\", Springer, 2009.\\n',\n",
       "  '\\n',\n",
       "  '    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\\n',\n",
       "  '           http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\\n',\n",
       "  '\\n',\n",
       "  '    Examples\\n',\n",
       "  '    --------\\n',\n",
       "  '    >>> from sklearn.datasets import load_boston\\n',\n",
       "  '    >>> from sklearn.model_selection import cross_val_score\\n',\n",
       "  '    >>> from sklearn.tree import DecisionTreeRegressor\\n',\n",
       "  '    >>> boston = load_boston()\\n',\n",
       "  '    >>> regressor = DecisionTreeRegressor(random_state=0)\\n',\n",
       "  '    >>> cross_val_score(regressor, boston.data, boston.target, cv=10)\\n',\n",
       "  '    ...                    # doctest: +SKIP\\n',\n",
       "  '    ...\\n',\n",
       "  '    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\\n',\n",
       "  '            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '    def __init__(self,\\n',\n",
       "  '                 criterion=\"mse\",\\n',\n",
       "  '                 splitter=\"best\",\\n',\n",
       "  '                 max_depth=None,\\n',\n",
       "  '                 min_samples_split=2,\\n',\n",
       "  '                 min_samples_leaf=1,\\n',\n",
       "  '                 min_weight_fraction_leaf=0.,\\n',\n",
       "  '                 max_features=None,\\n',\n",
       "  '                 random_state=None,\\n',\n",
       "  '                 max_leaf_nodes=None,\\n',\n",
       "  '                 min_impurity_decrease=0.,\\n',\n",
       "  '                 min_impurity_split=None,\\n',\n",
       "  '                 presort=False):\\n',\n",
       "  '        super(DecisionTreeRegressor, self).__init__(\\n',\n",
       "  '            criterion=criterion,\\n',\n",
       "  '            splitter=splitter,\\n',\n",
       "  '            max_depth=max_depth,\\n',\n",
       "  '            min_samples_split=min_samples_split,\\n',\n",
       "  '            min_samples_leaf=min_samples_leaf,\\n',\n",
       "  '            min_weight_fraction_leaf=min_weight_fraction_leaf,\\n',\n",
       "  '            max_features=max_features,\\n',\n",
       "  '            max_leaf_nodes=max_leaf_nodes,\\n',\n",
       "  '            random_state=random_state,\\n',\n",
       "  '            min_impurity_decrease=min_impurity_decrease,\\n',\n",
       "  '            min_impurity_split=min_impurity_split,\\n',\n",
       "  '            presort=presort)\\n',\n",
       "  '\\n',\n",
       "  '    def fit(self, X, y, sample_weight=None, check_input=True,\\n',\n",
       "  '            X_idx_sorted=None):\\n',\n",
       "  '        \"\"\"Build a decision tree regressor from the training set (X, y).\\n',\n",
       "  '\\n',\n",
       "  '        Parameters\\n',\n",
       "  '        ----------\\n',\n",
       "  '        X : array-like or sparse matrix, shape = [n_samples, n_features]\\n',\n",
       "  '            The training input samples. Internally, it will be converted to\\n',\n",
       "  '            ``dtype=np.float32`` and if a sparse matrix is provided\\n',\n",
       "  '            to a sparse ``csc_matrix``.\\n',\n",
       "  '\\n',\n",
       "  '        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\\n',\n",
       "  '            The target values (real numbers). Use ``dtype=np.float64`` and\\n',\n",
       "  \"            ``order='C'`` for maximum efficiency.\\n\",\n",
       "  '\\n',\n",
       "  '        sample_weight : array-like, shape = [n_samples] or None\\n',\n",
       "  '            Sample weights. If None, then samples are equally weighted. Splits\\n',\n",
       "  '            that would create child nodes with net zero or negative weight are\\n',\n",
       "  '            ignored while searching for a split in each node.\\n',\n",
       "  '\\n',\n",
       "  '        check_input : boolean, (default=True)\\n',\n",
       "  '            Allow to bypass several input checking.\\n',\n",
       "  \"            Don't use this parameter unless you know what you do.\\n\",\n",
       "  '\\n',\n",
       "  '        X_idx_sorted : array-like, shape = [n_samples, n_features], optional\\n',\n",
       "  '            The indexes of the sorted training input samples. If many tree\\n',\n",
       "  '            are grown on the same dataset, this allows the ordering to be\\n',\n",
       "  '            cached between trees. If None, the data will be sorted here.\\n',\n",
       "  \"            Don't use this parameter unless you know what to do.\\n\",\n",
       "  '\\n',\n",
       "  '        Returns\\n',\n",
       "  '        -------\\n',\n",
       "  '        self : object\\n',\n",
       "  '            Returns self.\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        super(DecisionTreeRegressor, self).fit(\\n',\n",
       "  '            X, y,\\n',\n",
       "  '            sample_weight=sample_weight,\\n',\n",
       "  '            check_input=check_input,\\n',\n",
       "  '            X_idx_sorted=X_idx_sorted)\\n',\n",
       "  '        return self\\n'],\n",
       " 873)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150000.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new results\n",
    "y_pred = regressor.predict(6.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15f6ceae2b0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the Decision Tree Regression Results\n",
    "X_grid = np.arange(min(X),max(X),0.001)\n",
    "plt.figure(figsize=[15,8])\n",
    "plt.scatter(X, y, color='r')\n",
    "plt.plot(X_grid , regressor.predict(X_grid.reshape(len(X_grid),1)), color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression is a bad fit for 1-D \n",
    "# but works well on highr dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
